{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import vit\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "\n",
    "PATCH_SIZE = 16\n",
    "\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "NUM_STEPS = 5000\n",
    "BATCHES_PER_STEP = 1\n",
    "EVAL_EVERY = 1000\n",
    "BASE_LR = 3e-3\n",
    "WEIGHT_DECAY = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.Lambda(lambda x: vit.image_to_patches(x, PATCH_SIZE).squeeze())\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR100(root='data', download=False, train=True, transform=transform)\n",
    "test_data = datasets.CIFAR100(root='data', download=False, train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'encoder.0.ffn_layer.model.0.weight',\n",
       "  'encoder.0.ffn_layer.model.3.weight',\n",
       "  'encoder.0.self_attention.in_proj_weight',\n",
       "  'encoder.0.self_attention.out_proj.weight',\n",
       "  'encoder.1.ffn_layer.model.0.weight',\n",
       "  'encoder.1.ffn_layer.model.3.weight',\n",
       "  'encoder.1.self_attention.in_proj_weight',\n",
       "  'encoder.1.self_attention.out_proj.weight',\n",
       "  'encoder.2.ffn_layer.model.0.weight',\n",
       "  'encoder.2.ffn_layer.model.3.weight',\n",
       "  'encoder.2.self_attention.in_proj_weight',\n",
       "  'encoder.2.self_attention.out_proj.weight',\n",
       "  'encoder.3.ffn_layer.model.0.weight',\n",
       "  'encoder.3.ffn_layer.model.3.weight',\n",
       "  'encoder.3.self_attention.in_proj_weight',\n",
       "  'encoder.3.self_attention.out_proj.weight',\n",
       "  'mlp_head.weight',\n",
       "  'patch_embedding.conv.weight'},\n",
       " {'class_token',\n",
       "  'encoder.0.ffn_layer.model.0.bias',\n",
       "  'encoder.0.ffn_layer.model.3.bias',\n",
       "  'encoder.0.ffn_norm.bias',\n",
       "  'encoder.0.ffn_norm.weight',\n",
       "  'encoder.0.self_att_norm.bias',\n",
       "  'encoder.0.self_att_norm.weight',\n",
       "  'encoder.0.self_attention.in_proj_bias',\n",
       "  'encoder.0.self_attention.out_proj.bias',\n",
       "  'encoder.1.ffn_layer.model.0.bias',\n",
       "  'encoder.1.ffn_layer.model.3.bias',\n",
       "  'encoder.1.ffn_norm.bias',\n",
       "  'encoder.1.ffn_norm.weight',\n",
       "  'encoder.1.self_att_norm.bias',\n",
       "  'encoder.1.self_att_norm.weight',\n",
       "  'encoder.1.self_attention.in_proj_bias',\n",
       "  'encoder.1.self_attention.out_proj.bias',\n",
       "  'encoder.2.ffn_layer.model.0.bias',\n",
       "  'encoder.2.ffn_layer.model.3.bias',\n",
       "  'encoder.2.ffn_norm.bias',\n",
       "  'encoder.2.ffn_norm.weight',\n",
       "  'encoder.2.self_att_norm.bias',\n",
       "  'encoder.2.self_att_norm.weight',\n",
       "  'encoder.2.self_attention.in_proj_bias',\n",
       "  'encoder.2.self_attention.out_proj.bias',\n",
       "  'encoder.3.ffn_layer.model.0.bias',\n",
       "  'encoder.3.ffn_layer.model.3.bias',\n",
       "  'encoder.3.ffn_norm.bias',\n",
       "  'encoder.3.ffn_norm.weight',\n",
       "  'encoder.3.self_att_norm.bias',\n",
       "  'encoder.3.self_att_norm.weight',\n",
       "  'encoder.3.self_attention.in_proj_bias',\n",
       "  'encoder.3.self_attention.out_proj.bias',\n",
       "  'encoder_norm.bias',\n",
       "  'encoder_norm.weight',\n",
       "  'mlp_head.bias',\n",
       "  'patch_embedding.conv.bias',\n",
       "  'pos_embedding'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = vit.VisionTransformer(\n",
    "    d_model = D_MODEL,\n",
    "    image_size = IMAGE_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    channels=CHANNELS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    d_ffn=4*D_MODEL,\n",
    "    num_encoder_layer=NUM_LAYERS,\n",
    "    num_classes=100,\n",
    "    p_dropout=0.1,\n",
    "    conv_embedding=True,\n",
    "    torch_attention=True\n",
    ")\n",
    "\n",
    "model.get_wd_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), BASE_LR, betas=(0.9, 0.999), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "lr_scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_loss(batch: list[torch.Tensor]) -> torch.Tensor:\n",
    "    model.train()\n",
    "    img, label = batch\n",
    "    img, label = img.to(device), label.to(device)\n",
    "    \n",
    "    pred = model(img)\n",
    "    loss = criterion(pred, label)\n",
    "    return loss\n",
    "\n",
    "def eval_model() -> float:\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for input, target in test_loader:\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            predicted = model.predict(input)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "loss_curve = []\n",
    "accuracy_curve = []\n",
    "\n",
    "num_epochs = (NUM_STEPS // (len(train_loader) // BATCHES_PER_STEP)) + 1\n",
    "\n",
    "print('=========================================')\n",
    "print('Starting Traning')\n",
    "print(f'  -Using device: {device}')\n",
    "print(f'  -Number of parameters: {np.sum([p.numel() for p in model.parameters()])}')\n",
    "print(f'  -Number of steps: {NUM_STEPS}')\n",
    "print(f'  -Number of epochs: {num_epochs}')\n",
    "print(f'  -Batch size: {train_loader.batch_size}')\n",
    "print('=========================================')\n",
    "print()\n",
    "\n",
    "\n",
    "step = 0\n",
    "loss_history = []\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        \n",
    "        loss = calc_train_loss(batch)\n",
    "        \n",
    "        (loss / BATCHES_PER_STEP).backward()\n",
    "\n",
    "        loss_history.append(loss.detach().clone())\n",
    "\n",
    "        if (batch_index + 1) % BATCHES_PER_STEP == 0:\n",
    "            # Perform step\n",
    "            step += 1\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            if (step + 1) % 100 == 0:\n",
    "                # Track train loss every few steps\n",
    "                average_loss = torch.stack(loss_history).mean().item()\n",
    "                loss_curve.append(average_loss)\n",
    "                loss_history = []\n",
    "                print(f'Epoch: {epoch + 1:3d}/{num_epochs:3d}, Step {step + 1:5d}/{NUM_STEPS}, Loss: {average_loss:.4f}')\n",
    "\n",
    "            # After eval_every steps, calc accuracy\n",
    "            if step % EVAL_EVERY == 0:\n",
    "                accuracy = eval_model()\n",
    "                accuracy_curve.append(accuracy)\n",
    "                print('=========================================')\n",
    "                print(f'Epoch: {epoch + 1:3d}/{num_epochs:3d}, Step: {step + 1:5d}/{NUM_STEPS}, Accuracy: {accuracy * 100:.4f} %')\n",
    "                print('=========================================')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
