{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vit\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8bad8e7580>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = torch.zeros((1, 3, 32, 32))\n",
    "img[:, 0, :16, :16] = 1 # oben links rot\n",
    "img[:, 1, 16:, : 16] = 1 # unten links gr√ºn\n",
    "img[:, 2, :16, 16:] = 1\n",
    "\n",
    "img[:, 0, 16:, 16:] = 1\n",
    "img[:, 2, 16:, 16:] = 210 / 255\n",
    "\n",
    "plt.imshow(img.squeeze(0).permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 192])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARcAAAEYCAYAAAB2hbIBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAAsTAAALEwEAmpwYAAAECUlEQVR4nO3bMWojQRBA0a5F1/H9D+IDlXPZEgr2I9C8l3YHTQWfgmFmdw/A//bv3Q8APpO4AAlxARLiAiTEBUjcnp7OXOdT0u68dM9Mfpk5l5nJ7jGTO49mYnMBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiRmd9/9BuAD2VyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AInb09OZ6/wbsDsv3TOTX2bOZWaye8zkzqOZ2FyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgMTs7rvfAHwgmwuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXIHF7djhnLvNvwJ6dV+6ZyR/m+zIzOftlJvcezMTmAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkZnff/QbgA9lcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuACJ27PDOXOZfwP27Lxyz0z+MN+XmcnZLzO592AmNhcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQGJ2991vAD6QzQVIiAuQEBcgIS5AQlyAhLgAiR9VakElHG69fwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 288x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "patches = vit._image_to_patches(img, 8)\n",
    "print(patches.shape)\n",
    "vit.show_patches(patches, 3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 4, 8, 8, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARcAAAEYCAYAAAB2hbIBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAAsTAAALEwEAmpwYAAAECUlEQVR4nO3bMWojQRBA0a5F1/H9D+IDlXPZEgr2I9C8l3YHTQWfgmFmdw/A//bv3Q8APpO4AAlxARLiAiTEBUjcnp7OXOdT0u68dM9Mfpk5l5nJ7jGTO49mYnMBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiRmd9/9BuAD2VyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AInb09OZ6/wbsDsv3TOTX2bOZWaye8zkzqOZ2FyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgMTs7rvfAHwgmwuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXIHF7djhnLvNvwJ6dV+6ZyR/m+zIzOftlJvcezMTmAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkZnff/QbgA9lcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuACJ27PDOXOZfwP27Lxyz0z+MN+XmcnZLzO592AmNhcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQEJcgIS4AAlxARLiAiTEBUiIC5AQFyAhLkBCXICEuAAJcQES4gIkxAVIiAuQEBcgIS5AQlyAhLgACXEBEuICJMQFSIgLkBAXICEuQGJ2991vAD6QzQVIiAuQEBcgIS5AQlyAhLgAiR9VakElHG69fwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 288x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = img.unfold(2, 8, 8).unfold(3, 8, 8).permute(0,2,3,4,5,1)\n",
    "print(out.shape)\n",
    "out = out.reshape(img.size(0), 4**2 ,-1)\n",
    "vit.show_patches(out, 3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 16, 64]          12,288\n",
      "           Dropout-2               [-1, 17, 64]               0\n",
      "         LayerNorm-3               [-1, 17, 64]             128\n",
      "            Linear-4               [-1, 17, 64]           4,096\n",
      "            Linear-5               [-1, 17, 64]           4,096\n",
      "            Linear-6               [-1, 17, 64]           4,096\n",
      "           Softmax-7            [-1, 2, 17, 17]               0\n",
      "           Dropout-8            [-1, 2, 17, 17]               0\n",
      "ScaledDotProductAttention-9            [-1, 2, 17, 32]               0\n",
      "           Linear-10               [-1, 17, 64]           4,160\n",
      "MultiHeadAttention-11               [-1, 17, 64]               0\n",
      "          Dropout-12               [-1, 17, 64]               0\n",
      "        LayerNorm-13               [-1, 17, 64]             128\n",
      "           Linear-14               [-1, 17, 64]           4,160\n",
      "          Dropout-15               [-1, 17, 64]               0\n",
      "             GELU-16               [-1, 17, 64]               0\n",
      "           Linear-17               [-1, 17, 64]           4,160\n",
      "          Dropout-18               [-1, 17, 64]               0\n",
      "              FFN-19               [-1, 17, 64]               0\n",
      "TransformerEncoderLayer-20               [-1, 17, 64]               0\n",
      "        LayerNorm-21                   [-1, 64]             128\n",
      "           Linear-22                   [-1, 64]           4,160\n",
      "             GELU-23                   [-1, 64]               0\n",
      "           Linear-24                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 42,250\n",
      "Trainable params: 42,250\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.16\n",
      "Params size (MB): 0.16\n",
      "Estimated Total Size (MB): 0.33\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "own_vit = vit.VisionTransformer(64, 32, 8, 3, 2, 64, 1, 10)\n",
    "torchsummary.summary(own_vit, (16, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.vision_transformer import VisionTransformer\n",
    "\n",
    "torch_vit = VisionTransformer(\n",
    "    32, 8, 1, 2, 64, 64, 0.0, 0.0, 10\n",
    ")\n",
    "torch_vit(torch.randn((1, 3, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_220096/4057277040.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  p = torch.load('github_impls/ViT-CIFAR/weights/vit_c10_aa_ls.pth', map_location='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vit_github import ViT\n",
    "\n",
    "github_vit = ViT(mlp_hidden=384, patch_size=4)\n",
    "p = torch.load('github_impls/ViT-CIFAR/weights/vit_c10_aa_ls.pth', map_location='cpu')\n",
    "github_vit.load_state_dict({k[6:]: p[k] for k in p})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "transform = v2.Compose((\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize((0.4915, 0.4823, .4468), (0.2470, 0.2435, 0.2616))\n",
    "))\n",
    "\n",
    "val_loader = DataLoader(datasets.CIFAR10('data', train=False, transform=transform), batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6975\n",
      "Entropy: 1.30594736328125\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "github_vit.to(device)\n",
    "correct = 0\n",
    "total = 0\n",
    "entropy_sum = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input, target in val_loader:\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        logits = github_vit(input)\n",
    "        predicted = logits.argmax(dim=-1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        entropy_sum += torch.distributions.Categorical(logits=logits).entropy().sum().item()\n",
    "    train_accuracy = correct / total\n",
    "    train_entropy = entropy_sum / total\n",
    "\n",
    "print(f'Accuracy: {train_accuracy}')\n",
    "print(f'Entropy: {train_entropy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
